import cv2
import face_recognition
import os
import dlib
import time
import datetime
from tqdm import tqdm


def process_video_in_memory(video_path: str, reference_img_path: str, interval: int = 1, tolerance: float = 0.6):
    # 1. ê¸°ì¤€ ì–¼êµ´ ë¡œë”©
    print(f"[INFO] ê¸°ì¤€ ì–¼êµ´ ë¡œë”© ì¤‘: {reference_img_path}")
    reference_img = cv2.imread(reference_img_path)
    if reference_img is None:
        print("[ERROR] ê¸°ì¤€ ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨ (None ë°˜í™˜)")
        return

    rgb_img = cv2.cvtColor(reference_img, cv2.COLOR_BGR2RGB)
    print("[DEBUG] ê¸°ì¤€ ì´ë¯¸ì§€ shape:", rgb_img.shape)

    # 2. ì–¼êµ´ ê°ì§€
    print("[INFO] ê¸°ì¤€ ì–¼êµ´ ê°ì§€ ì¤‘...")
    start = time.time()
    face_locations = face_recognition.face_locations(
        rgb_img, model="hog", number_of_times_to_upsample=0)
    end = time.time()
    print(f"[DEBUG] ê°ì§€ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {round(end - start, 2)}ì´ˆ")
    print("[DEBUG] ê°ì§€ëœ ì–¼êµ´ ê°œìˆ˜:", len(face_locations))
    if not face_locations:
        print("âŒ ê¸°ì¤€ ì–¼êµ´ì—ì„œ ì–¼êµ´ì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    # 3. ì–¼êµ´ ì¸ì½”ë”©
    print("[INFO] ì–¼êµ´ ì¸ì½”ë”© ì¤‘...")
    try:
        reference_encoding = face_recognition.face_encodings(
            rgb_img, face_locations)[0]
        print("[INFO] ê¸°ì¤€ ì–¼êµ´ ì¸ì½”ë”© ì™„ë£Œ")
    except Exception as e:
        print("[ERROR] ì–¼êµ´ ì¸ì½”ë”© ì‹¤íŒ¨:", e)
        return

    # 4. ì˜ìƒ ì—´ê¸°
    print(f"[INFO] ì˜ìƒ ë¡œë”© ì¤‘: {video_path}")
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("[ERROR] ì˜ìƒ ì—´ê¸° ì‹¤íŒ¨:", video_path)
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_interval = int(fps * interval)

    os.makedirs("matched_faces", exist_ok=True)

    frame_count = 0
    match_count = 0

    progress = tqdm(total=total_frames, desc="ğŸ ì˜ìƒ ë¶„ì„ ì¤‘", unit="frame")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            face_locations = face_recognition.face_locations(
                rgb_frame, model="hog", number_of_times_to_upsample=0)
            face_encodings = face_recognition.face_encodings(
                rgb_frame, face_locations)

            for encoding, (top, right, bottom, left) in zip(face_encodings, face_locations):
                distance = face_recognition.face_distance(
                    [reference_encoding], encoding)[0]

                if distance < tolerance:
                    time_position_sec = frame_count / fps
                    time_str = str(datetime.timedelta(
                        seconds=int(time_position_sec)))
                    print(f"\nâœ… ì–¼êµ´ ë§¤ì¹­ ì„±ê³µ: {time_str} ìœ„ì¹˜ (ê±°ë¦¬: {distance:.4f})")

                    save_path = os.path.join(
                        "matched_faces", f"frame_{frame_count}.jpg")
                    cv2.imwrite(save_path, frame)
                    print(f"[ì €ì¥ ì™„ë£Œ] {save_path}")
                    match_count += 1
                else:
                    print(f"\nâš ï¸ ìœ ì‚¬í•˜ì§€ë§Œ ê¸°ì¤€ ê±°ë¦¬ ì´ˆê³¼ (ê±°ë¦¬: {distance:.4f}) â†’ ë¬´ì‹œ")

        frame_count += 1
        progress.update(1)

    cap.release()
    progress.close()
    print(f"\n[ì™„ë£Œ] ì´ ë§¤ì¹­ëœ í”„ë ˆì„ ìˆ˜: {match_count}ì¥")


if __name__ == "__main__":
    # video_path = "C:/mini/videos/2025-04-11_[[ë‘ì‚° vs LG]]_ê²½ê¸°.mp4"
    video_path = "C:/mini/downloads/20250512.mp4"
    reference_img_path = "C:/mini/sample.jpg"

    process_video_in_memory(
        video_path=video_path,
        reference_img_path=reference_img_path,
        interval=1,  # ì´ˆë‹¹ 1í”„ë ˆì„ ê²€ì‚¬
        tolerance=0.45
    )
